{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import faiss\n",
    "from transformers import XLNetTokenizer, XLNetModel, MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath):\n",
    "    data = pd.read_csv(filepath, low_memory=False)\n",
    "    data = data[\"abstract\"].dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = load_and_preprocess_data(filepath=\"../../dataset/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_vector(text, model, tokenizer, max_chunk_length=512, overlap=50):\n",
    "    # Tokenize the text into chunks with specified overlap\n",
    "    tokens = tokenizer.tokenize(\n",
    "        tokenizer.decode(tokenizer.encode(text, add_special_tokens=True))\n",
    "    )\n",
    "    chunks = [\n",
    "        tokens[i : i + max_chunk_length]\n",
    "        for i in range(0, len(tokens), max_chunk_length - overlap)\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty tensor to store the embeddings\n",
    "    embeddings = torch.zeros((1, len(tokens), model.config.hidden_size))\n",
    "\n",
    "    # Iterate through chunks and generate embeddings\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(chunk)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([input_ids]))\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            embeddings[:, i : i + len(chunk), :] = last_hidden_states\n",
    "\n",
    "    # Average the embeddings over all tokens\n",
    "    document_vector = embeddings.mean(dim=1).squeeze().detach().numpy()\n",
    "    # Normalize the document vector\n",
    "    document_vector = document_vector / np.linalg.norm(document_vector)\n",
    "    return document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(source_vectors):\n",
    "    \"\"\"\n",
    "    This function builds a FAISS index for efficient similarity search.\n",
    "\n",
    "    Args:\n",
    "        source_vectors (np.ndarray): A numpy array containing source document vectors.\n",
    "\n",
    "    Returns:\n",
    "        faiss.Index: The built FAISS index object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose an appropriate FAISS index type based on your data and needs.\n",
    "    # This example uses IndexFlatL2 for simplicity. You might explore other options\n",
    "    # from the FAISS library.\n",
    "    index = faiss.IndexFlatL2(source_vectors.shape[1])\n",
    "\n",
    "    # Train the FAISS index on the source vectors\n",
    "    index.add(source_vectors)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(document_vector, source_vectors, threshold):\n",
    "    # Check if FAISS index exists\n",
    "    try:\n",
    "        # Assuming FAISS index is stored in a pickle file\n",
    "        with open(\"faiss_index.pkl\", \"rb\") as f:\n",
    "            index = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # If index doesn't exist, build it from source vectors\n",
    "        print(\"FAISS index not found. Building a new index...\")\n",
    "        index = build_faiss_index(source_vectors)\n",
    "        # Save the built index for future use\n",
    "        with open(\"faiss_index.pkl\", \"wb\") as f:\n",
    "            pickle.dump(index, f)\n",
    "\n",
    "    # Search for nearest neighbors using FAISS\n",
    "    D, I = index.search(document_vector.reshape(1, -1), 1)  # Search for 1 neighbor\n",
    "\n",
    "    most_similar_index = I.flatten()[0]\n",
    "    most_similar_score = D.flatten()[0]\n",
    "    most_similar_article = source_data[most_similar_index]\n",
    "\n",
    "    is_matched = most_similar_score > threshold\n",
    "\n",
    "    return [is_matched, most_similar_score, most_similar_article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(is_matched, similarity_scores, document, most_similar_article):\n",
    "    print(\"Analysis Results:\\n\")\n",
    "    print(f\"Similarity Score: {similarity_scores}\")\n",
    "    print(f\"Decision: {'Match Detected' if is_matched else 'No match Detected'}\")\n",
    "    print(f\"Article submitted: \\n{document}\")\n",
    "    if is_matched:\n",
    "        print(f\"Most Similar Article:\\n{most_similar_article}\")\n",
    "    else:\n",
    "        print(\"\\nNo evidence of similar document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"abstracts_embeddings_XLNet.pkl\", \"rb\") as f:\n",
    "#     source_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_vectors(data, model, tokenizer):\n",
    "    vectors = []\n",
    "    for text in tqdm(data[:100]):\n",
    "        vector = generate_document_vector(text, model, tokenizer)\n",
    "        vectors.append(vector)\n",
    "        # Normalize all vectors after creation\n",
    "    vectors = np.array(vectors)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "source_vectors = create_bert_vectors(source_data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectors = pd.DataFrame(source_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:\n",
      "\n",
      "Similarity Score: 2148.67578125\n",
      "Decision: Match Detected\n",
      "Article submitted: \n",
      "The Bhagavad Gita, a revered Hindu scripture, unfolds as a dialogue between Lord Krishna and the warrior prince Arjuna on the battlefield of Kurukshetra. Spanning 700 verses, it encapsulates profound teachings on duty (dharma), righteousness, and the path to spiritual realization. Krishna imparts wisdom on fulfilling one's responsibilities without attachment to the results, emphasizing the pursuit of selflessness and inner harmony. Themes of devotion, discipline, and the nature of existence resonate throughout, offering guidance on navigating life's moral dilemmas and achieving spiritual enlightenment. The Gita's timeless wisdom continues to inspire seekers on the quest for deeper understanding and purpose.\n",
      "Most Similar Article:\n",
      "Epidemics such as viral haemorrhagic fevers, severe acute respiratory syndrome, Middle East respiratory syndrome coronavirus or yet unknown ones have few chances of disappearing. Globalization, worldwide travel, climate change, social conflicts and wars, among others, are likely to favor the emergence of epidemics. Preparedness of hospitals to prevent the spread of these outbreaks is among the prioritized political programmes of many countries. The EuroNHID network has in the past drawn a map of features and equipment of hospitals across Europe to take care of highly contagious patients. We update the data regarding isolation capabilities and recommendations, with an emphasis on Mediterranean countries.\n"
     ]
    }
   ],
   "source": [
    "def run_plagiarism_checker(document, threshold=0.8):\n",
    "\n",
    "    # Generate vector for the document\n",
    "    document_vector = generate_document_vector(document, model, tokenizer)\n",
    "    # Perform plagiarism analysis\n",
    "    response = analysis(document_vector, source_vectors, threshold)\n",
    "\n",
    "    # Report results and get the plagiarism decision dictionary\n",
    "    report_results(response[0], response[1], document, response[2])\n",
    "\n",
    "document_to_check = \"The Bhagavad Gita, a revered Hindu scripture, unfolds as a dialogue between Lord Krishna and the warrior prince Arjuna on the battlefield of Kurukshetra. Spanning 700 verses, it encapsulates profound teachings on duty (dharma), righteousness, and the path to spiritual realization. Krishna imparts wisdom on fulfilling one's responsibilities without attachment to the results, emphasizing the pursuit of selflessness and inner harmony. Themes of devotion, discipline, and the nature of existence resonate throughout, offering guidance on navigating life's moral dilemmas and achieving spiritual enlightenment. The Gita's timeless wisdom continues to inspire seekers on the quest for deeper understanding and purpose.\"\n",
    "\n",
    "run_plagiarism_checker(document_to_check, 800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
