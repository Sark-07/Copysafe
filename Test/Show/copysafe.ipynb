{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to load and preprocess data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data['abstract'].dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prime\\AppData\\Local\\Temp\\ipykernel_14916\\2076912401.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "source_data = load_and_preprocess_data(filepath = \"../../dataset/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to generate document vector**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_vector(text, model, tokenizer, max_chunk_length=512, overlap=50):\n",
    "    # Tokenize the text into chunks with specified overlap\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text, add_special_tokens=True)))\n",
    "    chunks = [tokens[i:i + max_chunk_length] for i in range(0, len(tokens), max_chunk_length - overlap)]\n",
    "\n",
    "    # Initialize an empty tensor to store the embeddings\n",
    "    embeddings = torch.zeros((1, len(tokens), model.config.hidden_size))\n",
    "\n",
    "    # Iterate through chunks and generate embeddings\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(chunk)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([input_ids]))\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        embeddings[:, i:i + len(chunk), :] = last_hidden_states\n",
    "\n",
    "    # Average the embeddings over all tokens\n",
    "    document_vector = embeddings.mean(dim=1).squeeze().detach().numpy()\n",
    "    # Normalize the document vector\n",
    "    document_vector = document_vector / np.linalg.norm(document_vector)\n",
    "    return document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to create vectors from BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_vectors(data, model, tokenizer):\n",
    "    vectors = []\n",
    "    for text in tqdm(data[:100]):\n",
    "        vector = generate_document_vector(text, model, tokenizer)\n",
    "        vectors.append(vector)\n",
    "    # Normalize all vectors after creation\n",
    "    vectors = np.array(vectors)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to perform plagiarism analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagiarism_analysis(document_vector, vector_index, source_data, threshold):\n",
    "    similarity_scores = cosine_similarity([document_vector], vector_index)\n",
    "    is_plagiarism = any(score > threshold for score in similarity_scores[0])\n",
    "\n",
    "    if is_plagiarism:\n",
    "        most_similar_index = np.argmax(similarity_scores[0])\n",
    "        most_similar_article = source_data[most_similar_index]\n",
    "    else:\n",
    "        most_similar_article = None\n",
    "\n",
    "    return [is_plagiarism, sorted(similarity_scores[0], reverse=True)[0], most_similar_article]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to report results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(is_plagiarism, similarity_scores, document, most_similar_article):\n",
    "\n",
    "    print(\"Plagiarism Analysis Results:\\n\")\n",
    "    print(f\"Similarity Score: {similarity_scores}\")\n",
    "    print(\n",
    "        f\"Plagiarism Decision: {'Plagiarism Detected' if is_plagiarism else 'No Plagiarism Detected'}\"\n",
    "    )\n",
    "    print(f\"Article submitted: \\n{document}\")\n",
    "    if is_plagiarism:\n",
    "        print(f\"Most Similar Article:\\n{most_similar_article}\")\n",
    "    else:\n",
    "        print(\"\\nNo evidence of plagiarism.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main function to run the plagiarism checker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "# Generate vectors for source data\n",
    "source_vectors = create_bert_vectors(source_data, model, tokenizer)\n",
    "\n",
    "def run_plagiarism_checker(document, threshold=0.8):\n",
    "\n",
    "    # Generate vector for the document\n",
    "    document_vector = generate_document_vector(document, model, tokenizer)\n",
    "    # Perform plagiarism analysis\n",
    "    response = plagiarism_analysis(document_vector, source_vectors, source_data, threshold)\n",
    "\n",
    "    # Report results and get the plagiarism decision dictionary\n",
    "    report_results(response[0], response[1], document, response[2])\n",
    "\n",
    "document_to_check = \"The Bhagavad Gita, a revered Hindu scripture, unfolds as a dialogue between Lord Krishna and the warrior prince Arjuna on the battlefield of Kurukshetra. Spanning 700 verses, it encapsulates profound teachings on duty (dharma), righteousness, and the path to spiritual realization. Krishna imparts wisdom on fulfilling one's responsibilities without attachment to the results, emphasizing the pursuit of selflessness and inner harmony. Themes of devotion, discipline, and the nature of existence resonate throughout, offering guidance on navigating life's moral dilemmas and achieving spiritual enlightenment. The Gita's timeless wisdom continues to inspire seekers on the quest for deeper understanding and purpose.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run the checker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plagiarism Analysis Results:\n",
      "\n",
      "Similarity Score: 0.9776790142059326\n",
      "Plagiarism Decision: Plagiarism Detected\n",
      "Article submitted: \n",
      "The Bhagavad Gita, a revered Hindu scripture, unfolds as a dialogue between Lord Krishna and the warrior prince Arjuna on the battlefield of Kurukshetra. Spanning 700 verses, it encapsulates profound teachings on duty (dharma), righteousness, and the path to spiritual realization. Krishna imparts wisdom on fulfilling one's responsibilities without attachment to the results, emphasizing the pursuit of selflessness and inner harmony. Themes of devotion, discipline, and the nature of existence resonate throughout, offering guidance on navigating life's moral dilemmas and achieving spiritual enlightenment. The Gita's timeless wisdom continues to inspire seekers on the quest for deeper understanding and purpose.\n",
      "Most Similar Article:\n",
      "BACKGROUND AND OBJECTIVES: The daily incidence and deaths of coronavirus disease 2019 (COVID-19) in the USA are poorly understood. Internet search interest was found to be correlated with COVID-19 daily incidence in China, but has not yet been applied to the USA. Therefore, we examined the association of internet search-interest with COVID-19 daily incidence and deaths in the USA. METHODS: We extracted COVID-19 daily new cases and deaths in the USA from two population-based datasets, namely 1-point-3-acres.com and the Johns Hopkins COVID-19 data repository. The internet search-interest of COVID-19-related terms was obtained using Google Trends. The Pearson correlation test and general linear model were used to examine correlations and predict trends, respectively. RESULTS: There were 636,282 new cases and,325 deaths of COVID-19 in the USA from March 1 to April 15, 2020, with a crude mortality of 4.45%. The daily new cases peaked at 35,098 cases on April 10, 2020 and the daily deaths peaked at 2,494 on April 15, 2020. The search interest of COVID, “COVID pneumonia” and “COVID heart” were correlated with COVID-19 daily incidence, with 12 or 14 days of delay (Pearson’s r = 0.978, 0.978 and 0.979, respectively) and deaths with 19 days of delay (Pearson’s r = 0.963, 0.958 and 0.970, respectively). The 7-day follow-up with prospectively collected data showed no significant correlations of the observed data with the predicted daily new cases or daily deaths, using search interest of COVID, COVID heart, and COVID pneumonia. CONCLUSIONS: Search terms related to COVID-19 are highly correlated with the COVID-19 daily new cases and deaths in the USA.\n"
     ]
    }
   ],
   "source": [
    "run_plagiarism_checker(document_to_check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
